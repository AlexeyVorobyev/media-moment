# Лабораторная работа № 6

### Распознавание цифр с применением MLP

Для выполнения 1-ого задания используется нейронная сеть **MLP**, которая обучается с использованием библиотеки keras

**MLP** - Mutilayer perceptron, или Многослойный перцептрон, это перцептрон, который включает в себя скрытые слои,
помимо входного и выходного

**Перцептрон** — это базовая модель искусственного нейрона, разработанная для выполнения задач классификации и
распознавания образов.
Он является одним из первых алгоритмов машинного обучения,
основанных на принципах работы человеческого мозга,
и был предложен Фрэнком Розенблаттом в 1958 году.

![image](images/1.jpeg)

Было выполнено построение модели **многослойного перцептрона** для обработки изображений цифр с **MNIST**

Описание **архитектуры** построенной модели:

#### **1. Входной слой**

> Размер входа:
> 28 × 28 = 784 (размер одного изображения из MNIST после преобразования в одномерный массив).
>
> Входное изображение передается в виде одномерного массива длиной
> 784, где каждый элемент представляет собой нормализованное значение яркости пикселя
(диапазон от 0.0 до 1.0)

#### **2. Первый полносвязный слой**

> Количество нейронов: 512.
>
> Функция активации: ReLU (Rectified Linear Unit).
> Преобразует взвешенную сумму входов по формуле:
> f(x)=max(0,x).
>
> Преимущество ReLU: предотвращает проблему исчезающего градиента и ускоряет обучение.
> Каждый из 512 нейронов подключён ко всем 784 входам.

_Статья хабр о проблеме исчезающего градиента_ - https://habr.com/ru/articles/462381/

#### **3. Первый Dropout-слой**

> Вероятность отключения нейронов: 20% (0.2).
>
> На каждой итерации обучения случайно отключается 20% нейронов, чтобы предотвратить переобучение.

![image](images/2.png)

_Статья хабр о dropout_ - https://habr.com/ru/companies/wunderfund/articles/330814/

Сети для обучения получаются с помощью исключения из сети (dropping out) нейронов с вероятностью $p$, таким образом, вероятность того, что нейрон останется в сети, составляет $q=1-p$. “Исключение” нейрона означает, что при любых входных данных или параметрах он возвращает 0.

#### **4. Второй полносвязный слой**

> Количество нейронов: 512.
>
> Функция активации: ReLU (Rectified Linear Unit).
> Преобразует взвешенную сумму входов по формуле:
> f(x)=max(0,x).

#### **5. Второй Dropout-слой**

> Вероятность отключения нейронов: 20% (0.2).
>
> На каждой итерации обучения случайно отключается 20% нейронов, чтобы предотвратить переобучение.

#### **6. Выходной слой**

> Количество нейронов: 10 (по числу классов цифр от 0 до 9).
> Роль: Предсказать вероятность принадлежности входного изображения к одному из 10 классов.

# Результаты классификации цифр на датасете MNIST

## Многослойный перцептрон

| Количество эпох | Точность | Потеря | Время обучения (сек) | Время проверки (сек) |
|-----------------|----------|--------|----------------------|----------------------| 
| 1               | 0.9585   | 0.1255 | 5.629220             | 1                    |
| 2               | 0.9684   | 0.1023 | 6.325084             | 1                    |
| 5               | 0.9707   | 0.0945 | 13.133765            | 1                    |
| 20              | 0.9801   | 0.0937 | 46.253127            | 1                    |
| 50              | 0.9835   | 0.1043 | 103.562231           | 1                    |
| 100             | 0.9832   | 0.1806 | 205.426771           | 1                    |
| 1000            | 0.9830   | 0.5230 | 1016.115069          | 1                    |

### Вывод:

Видим, что чрезмерное повышение количества эпох не приводит к существенному увеличению точности модели,
но повышает значение функции потери

Феномен того, что при такой же +- точности увеличивается значение функции потери объясняется тем, 
что модель делает менее уверенные высказывания на корректных данных (в связи с переобучением) 
и более уверенные на некорректных данных (вероятно к этому случаю это не относится)
