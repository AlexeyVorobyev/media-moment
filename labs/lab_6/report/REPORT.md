# Лабораторная работа № 6

### Распознавание цифр с применением MLP

Для выполнения 1-ого задания используется нейронная сеть **MLP**, которая обучается с использованием библиотеки keras

**MLP** - Mutilayer perceptron, или Многослойный перцептрон, это перцептрон, который включает в себя скрытые слои,
помимо входного и выходного

**Перцептрон** — это базовая модель искусственного нейрона, разработанная для выполнения задач классификации и
распознавания образов.
Он является одним из первых алгоритмов машинного обучения,
основанных на принципах работы человеческого мозга,
и был предложен Фрэнком Розенблаттом в 1958 году.

**Эпоха** в обучении нейронной сети — это один полный проход по всему обучающему набору данных. Во время эпохи модель
обрабатывает все доступные данные и обновляет свои параметры (веса и смещения) на основе вычисленной ошибки.

#### Как работает эпоха:

> **Разбиение данных на батчи**:
> Обычно данные делятся на небольшие части (батчи), так как обработать весь набор данных за один раз может быть слишком
> ресурсоемко.
> Размер батча определяется параметром batch_size.
>
> **Обработка каждого батча**:
> Для каждого батча модель делает предсказания, вычисляет функцию потерь и обновляет веса через алгоритм обратного
> распространения ошибки с использованием градиентного спуска.
>
> **Проход через весь набор данных**:
> После обработки всех батчей эпоха завершается.

![image](images/1.jpeg)

Было выполнено построение модели **многослойного перцептрона** для обработки изображений цифр с **MNIST**

# Описание **архитектур** построенных моделей:

## Многослойный перцептрон:

### **1. Входной слой**

> Размер входа:
> 28 × 28 = 784 (размер одного изображения из MNIST после преобразования в одномерный массив).
>
> Входное изображение передается в виде одномерного массива длиной
> 784, где каждый элемент представляет собой нормализованное значение яркости пикселя
(диапазон от 0.0 до 1.0)

### **2. Первый полносвязный слой**

> Количество нейронов: 512.
>
> Функция активации: ReLU (Rectified Linear Unit).
> Преобразует взвешенную сумму входов по формуле:
> f(x)=max(0,x).
>
> Преимущество ReLU: предотвращает проблему исчезающего градиента и ускоряет обучение.
> Каждый из 512 нейронов подключён ко всем 784 входам.

_Статья хабр о проблеме исчезающего градиента_ - https://habr.com/ru/articles/462381/

### **3. Первый Dropout-слой**

> Вероятность отключения нейронов: 20% (0.2).
>
> На каждой итерации обучения случайно отключается 20% нейронов, чтобы предотвратить переобучение.

![image](images/2.png)

_Статья хабр о dropout_ - https://habr.com/ru/companies/wunderfund/articles/330814/

Сети для обучения получаются с помощью исключения из сети (dropping out) нейронов с вероятностью $p$, таким образом,
вероятность того, что нейрон останется в сети, составляет $q=1-p$. “Исключение” нейрона означает, что при любых входных
данных или параметрах он возвращает 0.

### **4. Второй полносвязный слой**

> Количество нейронов: 512.
>
> Функция активации: ReLU (Rectified Linear Unit).
> Преобразует взвешенную сумму входов по формуле:
> f(x)=max(0,x).

### **5. Второй Dropout-слой**

> Вероятность отключения нейронов: 20% (0.2).
>
> На каждой итерации обучения случайно отключается 20% нейронов, чтобы предотвратить переобучение.

### **6. Выходной слой**

> Количество нейронов: 10 (по числу классов цифр от 0 до 9).
> Роль: Предсказать вероятность принадлежности входного изображения к одному из 10 классов.

## Свёрточная нейронная сеть:

### **1. Входной слой**

> Форма входа: (width, height, depth).
>
> width и height: размеры изображения (например, 28x28 для MNIST).
>
> depth: количество каналов (например, 1 для черно-белых изображений или 3 для цветных RGB-изображений).

### **2. Первичный свёрточный слой**

> Проводи непосредственную свёртку изображения используя ядро (3x3) в нашем случае
> filters: int, the dimension of the output space (the number of filters in the convolution).
>
> Фильтр - это некоторое ядро свёртки, которое выявляет некоторый признак
>
> На выходе имеем карты признаков по каждому фильтру

### **3. Слой подвыборки (MaxPooling2D)**

> Размер окна: 2×2.
>
> Роль - Уменьшение пространственного размера карт признаков (в 2 раза по ширине и высоте).
>
> Результат - Каждая карта признаков становится вдвое меньше по размеру

### **4. Слой выравнивания (Flatten)**

> Преобразует двумерные карты признаков в одномерный вектор.
>
> Результат - Одномерный вектор, длина которого зависит от размера карт признаков после свертки и пуллинга.

### **5. Полносвязный слой**

> Количество нейронов: 512.
>
> Функция активации: ReLU (Rectified Linear Unit).
> Преобразует взвешенную сумму входов по формуле:
> f(x)=max(0,x).

### **6. Dropout-слой**

> Вероятность отключения нейронов: 20% (0.2).
>
> На каждой итерации обучения случайно отключается 20% нейронов, чтобы предотвратить переобучение.

### **7. Выходной слой**

> Количество нейронов: 10 (по числу классов цифр от 0 до 9).
> Роль: Предсказать вероятность принадлежности входного изображения к одному из 10 классов.

# Результаты классификации цифр на датасете MNIST

## Многослойный перцептрон

| Количество эпох | Точность | Потеря | Время обучения (сек) | Время проверки (сек) |
|-----------------|----------|--------|----------------------|----------------------| 
| 1               | 0.9585   | 0.1255 | 5.629220             | 1                    |
| 2               | 0.9684   | 0.1023 | 6.325084             | 1                    |
| 5               | 0.9707   | 0.0945 | 13.133765            | 1                    |
| 20              | 0.9801   | 0.0937 | 46.253127            | 1                    |
| 50              | 0.9835   | 0.1043 | 103.562231           | 1                    |
| 100             | 0.9832   | 0.1806 | 205.426771           | 1                    |
| 1000            | 0.9830   | 0.5230 | 1016.115069          | 1                    |

### Вывод:

Видим, что чрезмерное повышение количества эпох не приводит к существенному увеличению точности модели,
но повышает значение функции потери

**Функция потерь** - кросс-энтропия

Кросс-энтропия измеряет разницу между истинным распределением классов и распределением, предсказанным моделью. Основная
цель модели — минимизировать эту разницу.

![image](images/3.png)

Феномен того, что при такой же +- точности увеличивается значение функции потери объясняется тем,
что модель делает менее уверенные высказывания на корректных данных (в связи с переобучением)
и более уверенные на некорректных данных (вероятно к этому случаю это не относится)

## Свёрточная нейронная сеть

| Количество эпох | Точность | Потеря | Время обучения (сек) | Время проверки (сек) |
|-----------------|----------|--------|----------------------|----------------------| 
| 1               | 0.9736   | 0.0751 | 12.923501            | 1                    |
| 2               | 0.9792   | 0.0566 | 22.098835            | 1                    |
| 5               | 0.9815   | 0.0487 | 52.847397            | 1                    |
| 20              | 0.9849   | 0.0685 | 192.872451           | 1                    |
| 50              | 0.9848   | 0.1312 | 485.929832           | 1                    |
| 100             | 0.9875   | 0.1414 | 952.781081           | 1                    |

### Вывод:

Видим, что чрезмерное повышение количества эпох не приводит к существенному увеличению точности модели,
но повышает значение функции потери


## Общий вывод:

Исходя из результата обучения и проверки двух моделей сделаем выводы:

1. CNN справляется с распознаванием лучше, чем традиционный MLP
2. CNN обучать дольше, чем традиционный MLP
3. Мера потери значительно ниже у CNN
4. Скорость работы не отличается
